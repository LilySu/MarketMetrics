{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts from r/python...\n",
      "Fetching comments for 6 posts...\n",
      "Fetching comments for post 3/6\n",
      "Fetching comments for post 4/6\n",
      "Fetching comments for post 5/6\n",
      "Fetching comments for post 7/6\n",
      "Fetching comments for post 8/6\n",
      "Fetching comments for post 10/6\n",
      "\n",
      "Collected 6 posts and 285 comments\n",
      "\n",
      "Example comment thread structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>is_video</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1gcq5rg</td>\n",
       "      <td>Configuration format</td>\n",
       "      <td>37</td>\n",
       "      <td>2024-10-26 10:24:37</td>\n",
       "      <td>49</td>\n",
       "      <td>https://www.reddit.com/r/Python/comments/1gcq5...</td>\n",
       "      <td>I currently use JSONs for storing my configura...</td>\n",
       "      <td>Messmer_Impaler</td>\n",
       "      <td>0.89</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Python/comments/1gcq5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1gcob89</td>\n",
       "      <td>Effortlessly Monitor and Manage Concurrent Fun...</td>\n",
       "      <td>21</td>\n",
       "      <td>2024-10-26 09:00:56</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/Python/comments/1gcob...</td>\n",
       "      <td>**What my project does**\\n\\n`functionmonitor` ...</td>\n",
       "      <td>Vivid-Job2568</td>\n",
       "      <td>0.88</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Python/comments/1gcob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  thread_id                                              title  score  \\\n",
       "2   1gcq5rg                               Configuration format     37   \n",
       "3   1gcob89  Effortlessly Monitor and Manage Concurrent Fun...     21   \n",
       "\n",
       "          created_utc  num_comments  \\\n",
       "2 2024-10-26 10:24:37            49   \n",
       "3 2024-10-26 09:00:56             7   \n",
       "\n",
       "                                                 url  \\\n",
       "2  https://www.reddit.com/r/Python/comments/1gcq5...   \n",
       "3  https://www.reddit.com/r/Python/comments/1gcob...   \n",
       "\n",
       "                                            selftext           author  \\\n",
       "2  I currently use JSONs for storing my configura...  Messmer_Impaler   \n",
       "3  **What my project does**\\n\\n`functionmonitor` ...    Vivid-Job2568   \n",
       "\n",
       "   upvote_ratio  is_video                                          permalink  \n",
       "2          0.89     False  https://www.reddit.com/r/Python/comments/1gcq5...  \n",
       "3          0.88     False  https://www.reddit.com/r/Python/comments/1gcob...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# import json\n",
    "\n",
    "# class RedditScraper:\n",
    "#     def __init__(self):\n",
    "#         self.headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "#         }\n",
    "        \n",
    "#     def fetch_subreddit_posts(self, subreddit, limit=25):\n",
    "#         \"\"\"\n",
    "#         Fetch posts from a subreddit using Reddit's JSON API\n",
    "        \n",
    "#         Args:\n",
    "#             subreddit (str): Name of the subreddit without the 'r/'\n",
    "#             limit (int): Maximum number of posts to fetch (default 25)\n",
    "        \n",
    "#         Returns:\n",
    "#             pandas.DataFrame: DataFrame containing post data\n",
    "#         \"\"\"\n",
    "#         posts_data = []\n",
    "#         after = None\n",
    "#         posts_collected = 0\n",
    "        \n",
    "#         while posts_collected < limit:\n",
    "#             url = f'https://www.reddit.com/r/{subreddit}/hot.json?limit=100'\n",
    "#             if after:\n",
    "#                 url += f'&after={after}'\n",
    "                \n",
    "#             try:\n",
    "#                 response = requests.get(url, headers=self.headers)\n",
    "#                 response.raise_for_status()\n",
    "#                 data = response.json()\n",
    "                \n",
    "#                 for post in data['data']['children']:\n",
    "#                     post_data = post['data']\n",
    "                    \n",
    "#                     posts_data.append({\n",
    "#                         'thread_id': post_data.get('id'),  # Unique identifier for the post\n",
    "#                         'title': post_data.get('title'),\n",
    "#                         'score': post_data.get('score'),\n",
    "#                         'created_utc': datetime.fromtimestamp(post_data.get('created_utc', 0)),\n",
    "#                         'num_comments': post_data.get('num_comments'),\n",
    "#                         'url': post_data.get('url'),\n",
    "#                         'selftext': post_data.get('selftext'),\n",
    "#                         'author': post_data.get('author'),\n",
    "#                         'upvote_ratio': post_data.get('upvote_ratio'),\n",
    "#                         'is_video': post_data.get('is_video', False),\n",
    "#                         'permalink': 'https://www.reddit.com' + post_data.get('permalink', '')\n",
    "#                     })\n",
    "                    \n",
    "#                     posts_collected += 1\n",
    "#                     if posts_collected >= limit:\n",
    "#                         break\n",
    "                \n",
    "#                 after = data['data'].get('after')\n",
    "#                 if not after:\n",
    "#                     break\n",
    "                    \n",
    "#                 time.sleep(2)\n",
    "                \n",
    "#             except requests.exceptions.RequestException as e:\n",
    "#                 print(f\"Error fetching posts: {e}\")\n",
    "#                 break\n",
    "                \n",
    "#         return pd.DataFrame(posts_data)\n",
    "\n",
    "#     def fetch_post_comments(self, post_id, permalink):\n",
    "#         \"\"\"\n",
    "#         Fetch comments for a specific post\n",
    "        \n",
    "#         Args:\n",
    "#             post_id (str): The ID of the post\n",
    "#             permalink (str): The permalink of the post\n",
    "        \n",
    "#         Returns:\n",
    "#             list: List of comment dictionaries\n",
    "#         \"\"\"\n",
    "#         comments_data = []\n",
    "        \n",
    "#         try:\n",
    "#             # Remove the domain from permalink and add .json\n",
    "#             json_url = permalink.replace('https://www.reddit.com', '') + '.json'\n",
    "#             url = f'https://www.reddit.com{json_url}'\n",
    "            \n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             response.raise_for_status()\n",
    "#             data = response.json()\n",
    "            \n",
    "#             # Process comments recursively\n",
    "#             def process_comments(comment_data, parent_id=None, level=0):\n",
    "#                 if isinstance(comment_data, dict):\n",
    "#                     if comment_data.get('kind') == 't1':  # t1 represents a comment\n",
    "#                         comment = comment_data['data']\n",
    "#                         comments_data.append({\n",
    "#                             'thread_id': post_id,  # Link to the parent post\n",
    "#                             'comment_id': comment.get('id'),\n",
    "#                             'parent_comment_id': parent_id,  # Track comment parent\n",
    "#                             'level': level,  # Track comment nesting level\n",
    "#                             'author': comment.get('author'),\n",
    "#                             'body': comment.get('body'),\n",
    "#                             'score': comment.get('score'),\n",
    "#                             'created_utc': datetime.fromtimestamp(comment.get('created_utc', 0))\n",
    "#                         })\n",
    "                        \n",
    "#                         # Process replies\n",
    "#                         replies = comment.get('replies')\n",
    "#                         if replies and isinstance(replies, dict):\n",
    "#                             for child in replies.get('data', {}).get('children', []):\n",
    "#                                 process_comments(child, comment.get('id'), level + 1)\n",
    "                    \n",
    "#                     # Process children if present\n",
    "#                     elif 'data' in comment_data and 'children' in comment_data['data']:\n",
    "#                         for child in comment_data['data']['children']:\n",
    "#                             process_comments(child, parent_id, level)\n",
    "            \n",
    "#             # Start processing from the comment tree\n",
    "#             if len(data) > 1:  # Make sure we have comments data\n",
    "#                 process_comments(data[1])  # data[0] is the post, data[1] is the comments\n",
    "                \n",
    "#             time.sleep(2)  # Be nice to Reddit's servers\n",
    "            \n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(f\"Error fetching comments for post {post_id}: {e}\")\n",
    "        \n",
    "#         return comments_data\n",
    "\n",
    "#     def scrape_subreddit(self, subreddit, post_limit=25, min_comments=5):\n",
    "#         \"\"\"\n",
    "#         Scrape both posts and comments from a subreddit\n",
    "        \n",
    "#         Args:\n",
    "#             subreddit (str): Subreddit name\n",
    "#             post_limit (int): Number of posts to fetch\n",
    "#             min_comments (int): Minimum number of comments for a post to be included\n",
    "        \n",
    "#         Returns:\n",
    "#             tuple: (posts DataFrame, comments DataFrame)\n",
    "#         \"\"\"\n",
    "#         print(f\"Fetching posts from r/{subreddit}...\")\n",
    "#         posts_df = self.fetch_subreddit_posts(subreddit, limit=post_limit)\n",
    "        \n",
    "#         # Filter posts with minimum comments\n",
    "#         posts_df = posts_df[posts_df['num_comments'] >= min_comments]\n",
    "        \n",
    "#         all_comments = []\n",
    "#         total_posts = len(posts_df)\n",
    "        \n",
    "#         print(f\"Fetching comments for {total_posts} posts...\")\n",
    "#         for idx, post in posts_df.iterrows():\n",
    "#             print(f\"Fetching comments for post {idx + 1}/{total_posts}\")\n",
    "#             comments = self.fetch_post_comments(post['thread_id'], post['permalink'])\n",
    "#             all_comments.extend(comments)\n",
    "        \n",
    "#         comments_df = pd.DataFrame(all_comments)\n",
    "        \n",
    "#         return posts_df, comments_df\n",
    "\n",
    "#     def save_data(self, posts_df, comments_df, subreddit):\n",
    "#         \"\"\"Save posts and comments to CSV and JSON formats\"\"\"\n",
    "#         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "#         # Save posts\n",
    "#         posts_csv = f'{subreddit}_posts_{timestamp}.csv'\n",
    "#         posts_json = f'{subreddit}_posts_{timestamp}.json'\n",
    "#         posts_df.to_csv(posts_csv, index=False)\n",
    "#         posts_df.to_json(posts_json, orient='records', date_format='iso')\n",
    "        \n",
    "#         # Save comments\n",
    "#         comments_csv = f'{subreddit}_comments_{timestamp}.csv'\n",
    "#         comments_json = f'{subreddit}_comments_{timestamp}.json'\n",
    "#         comments_df.to_csv(comments_csv, index=False)\n",
    "#         comments_df.to_json(comments_json, orient='records', date_format='iso')\n",
    "        \n",
    "#         print(f\"\\nData saved to:\")\n",
    "#         print(f\"Posts: {posts_csv} and {posts_json}\")\n",
    "#         print(f\"Comments: {comments_csv} and {comments_json}\")\n",
    "\n",
    "\n",
    "# scraper = RedditScraper()\n",
    "\n",
    "# # Configuration\n",
    "# SUBREDDIT = \"python\"\n",
    "# POST_LIMIT = 10  # Number of posts to fetch\n",
    "# MIN_COMMENTS = 5  # Only fetch posts with at least this many comments\n",
    "\n",
    "# # Scrape data\n",
    "# posts_df, comments_df = scraper.scrape_subreddit(\n",
    "#     SUBREDDIT,\n",
    "#     post_limit=POST_LIMIT,\n",
    "#     min_comments=MIN_COMMENTS\n",
    "# )\n",
    "\n",
    "# # Display basic stats\n",
    "# print(f\"\\nCollected {len(posts_df)} posts and {len(comments_df)} comments\")\n",
    "\n",
    "# # Show example of thread structure\n",
    "# if not comments_df.empty:\n",
    "#     print(\"\\nExample comment thread structure:\")\n",
    "#     sample_thread = comments_df[comments_df['thread_id'] == comments_df['thread_id'].iloc[0]]\n",
    "\n",
    "# posts_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>level</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1gcq5rg</td>\n",
       "      <td>ltvpp2i</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>swigganicks</td>\n",
       "      <td>I must ask, what kind of configuration file is...</td>\n",
       "      <td>80</td>\n",
       "      <td>2024-10-26 10:34:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gcq5rg</td>\n",
       "      <td>ltvqi6w</td>\n",
       "      <td>ltvpp2i</td>\n",
       "      <td>1</td>\n",
       "      <td>Messmer_Impaler</td>\n",
       "      <td>There's a repeating pattern to the configs whi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-10-26 10:38:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  thread_id comment_id parent_comment_id  level           author  \\\n",
       "0   1gcq5rg    ltvpp2i              None      0      swigganicks   \n",
       "1   1gcq5rg    ltvqi6w           ltvpp2i      1  Messmer_Impaler   \n",
       "\n",
       "                                                body  score  \\\n",
       "0  I must ask, what kind of configuration file is...     80   \n",
       "1  There's a repeating pattern to the configs whi...      1   \n",
       "\n",
       "          created_utc  \n",
       "0 2024-10-26 10:34:05  \n",
       "1 2024-10-26 10:38:24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comments_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for r/python (1/8)\n",
      "Fetching data for r/programming (2/8)\n",
      "Fetching data for r/datascience (3/8)\n",
      "Fetching data for r/machinelearning (4/8)\n",
      "Fetching data for r/javascript (5/8)\n",
      "Fetching data for r/webdev (6/8)\n",
      "Fetching data for r/technology (7/8)\n",
      "Fetching data for r/science (8/8)\n",
      "\n",
      "Data saved to:\n",
      "CSV: subreddit_info_20241026_174616.csv\n",
      "JSON: subreddit_info_20241026_174616.json\n",
      "\n",
      "Summary Statistics:\n",
      "Total subreddits analyzed: 8\n",
      "\n",
      "Subscriber Statistics:\n",
      "count    8.000000e+00\n",
      "mean     8.549066e+06\n",
      "std      1.118129e+07\n",
      "min      1.286093e+06\n",
      "25%      2.363017e+06\n",
      "50%      2.788873e+06\n",
      "75%      9.241988e+06\n",
      "max      3.299837e+07\n",
      "Name: subscribers, dtype: float64\n",
      "\n",
      "Active Users Statistics:\n",
      "count       8.000000\n",
      "mean      790.375000\n",
      "std      1074.701412\n",
      "min        95.000000\n",
      "25%       125.750000\n",
      "50%       173.000000\n",
      "75%      1054.000000\n",
      "max      2747.000000\n",
      "Name: active_users, dtype: float64\n",
      "\n",
      "Age of Subreddits:\n",
      "count       8.000000\n",
      "mean     5988.250000\n",
      "std       616.676055\n",
      "min      4830.000000\n",
      "25%      5706.750000\n",
      "50%      6119.000000\n",
      "75%      6235.000000\n",
      "max      6815.000000\n",
      "Name: age_days, dtype: float64\n",
      "\n",
      "Subreddit Types Distribution:\n",
      "subreddit_type\n",
      "public    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature Availability:\n",
      "allows_images: 0.0% enabled\n",
      "allows_videos: 0.0% enabled\n",
      "allows_polls: 0.0% enabled\n",
      "allow_galleries: 37.5% enabled\n",
      "wiki_enabled: 100.0% enabled\n",
      "emojis_enabled: 37.5% enabled\n",
      "\n",
      "Top 5 Subreddits by Subscribers:\n",
      "      display_name  subscribers  active_users\n",
      "7          science     32998368          2236\n",
      "6       technology     17318299          2747\n",
      "1      programming      6549884           660\n",
      "3  MachineLearning      2929181           132\n",
      "5           webdev      2648565           214\n",
      "\n",
      "Newest Subreddits:\n",
      "  display_name         created_utc\n",
      "1  programming 2006-02-28 10:19:29\n",
      "7      science 2006-10-18 06:54:26\n",
      "0       Python 2008-01-24 19:14:39\n",
      "6   technology 2008-01-24 19:27:55\n",
      "4   javascript 2008-01-24 22:32:16\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# import json\n",
    "\n",
    "# class SubredditScraper:\n",
    "#     def __init__(self):\n",
    "#         self.headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "#         }\n",
    "\n",
    "#     def get_subreddit_info(self, subreddit):\n",
    "#         \"\"\"\n",
    "#         Fetch detailed information about a specific subreddit\n",
    "        \n",
    "#         Args:\n",
    "#             subreddit (str): Subreddit name without 'r/'\n",
    "        \n",
    "#         Returns:\n",
    "#             dict: Dictionary containing subreddit information\n",
    "#         \"\"\"\n",
    "#         url = f'https://www.reddit.com/r/{subreddit}/about.json'\n",
    "        \n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             response.raise_for_status()\n",
    "#             data = response.json()['data']\n",
    "            \n",
    "#             # Extract all available attributes\n",
    "#             subreddit_info = {\n",
    "#                 'display_name': data.get('display_name'),\n",
    "#                 'title': data.get('title'),\n",
    "#                 'display_name_prefixed': data.get('display_name_prefixed'),\n",
    "#                 'subscribers': data.get('subscribers'),\n",
    "#                 'active_users': data.get('active_user_count'),\n",
    "#                 'created_utc': datetime.fromtimestamp(data.get('created_utc', 0)),\n",
    "#                 'description': data.get('description'),\n",
    "#                 'public_description': data.get('public_description'),\n",
    "#                 'subreddit_type': data.get('subreddit_type'),\n",
    "#                 'over18': data.get('over18'),\n",
    "#                 'is_default': data.get('default_set', False),\n",
    "#                 'allows_images': data.get('allows_images', False),\n",
    "#                 'allows_videos': data.get('allows_videos', False),\n",
    "#                 'allows_polls': data.get('allows_polls', False),\n",
    "#                 'allow_galleries': data.get('allow_galleries', False),\n",
    "#                 'allow_predictions': data.get('allow_predictions', False),\n",
    "#                 'spoilers_enabled': data.get('spoilers_enabled', False),\n",
    "#                 'comment_score_hide_mins': data.get('comment_score_hide_mins'),\n",
    "#                 'submission_type': data.get('submission_type'),\n",
    "#                 'restrict_posting': data.get('restrict_posting', False),\n",
    "#                 'restrict_commenting': data.get('restrict_commenting', False),\n",
    "#                 'lang': data.get('lang'),\n",
    "#                 'url': f\"https://www.reddit.com/r/{subreddit}/\",\n",
    "#                 'wiki_enabled': data.get('wiki_enabled', False),\n",
    "#                 'is_crosspostable': data.get('is_crosspostable', False),\n",
    "#                 'quarantine': data.get('quarantine', False),\n",
    "#                 'community_icon': data.get('community_icon'),\n",
    "#                 'banner_img': data.get('banner_img'),\n",
    "#                 'emojis_enabled': data.get('emojis_enabled', False),\n",
    "#                 'advertiser_category': data.get('advertiser_category'),\n",
    "#                 'public_traffic': data.get('public_traffic', False),\n",
    "#                 'subscribers_rank': data.get('subscribers_rank'),\n",
    "#                 'video_stream_enabled': data.get('video_stream_enabled', False),\n",
    "#                 'header_title': data.get('header_title'),\n",
    "#                 'can_assign_link_flair': data.get('can_assign_link_flair', False),\n",
    "#                 'can_assign_user_flair': data.get('can_assign_user_flair', False),\n",
    "#                 'user_flair_enabled': data.get('user_flair_enabled_in_sr', False),\n",
    "#                 'link_flair_enabled': data.get('link_flair_enabled', False),\n",
    "#                 'primary_color': data.get('primary_color'),\n",
    "#                 'key_color': data.get('key_color'),\n",
    "#                 'last_checked': datetime.now()\n",
    "#             }\n",
    "            \n",
    "#             return subreddit_info\n",
    "            \n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(f\"Error fetching data for r/{subreddit}: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def get_multiple_subreddits(self, subreddits, save_results=True):\n",
    "#         \"\"\"\n",
    "#         Fetch information for multiple subreddits\n",
    "        \n",
    "#         Args:\n",
    "#             subreddits (list): List of subreddit names\n",
    "#             save_results (bool): Whether to save results to files\n",
    "        \n",
    "#         Returns:\n",
    "#             pandas.DataFrame: DataFrame containing all subreddit information\n",
    "#         \"\"\"\n",
    "#         all_data = []\n",
    "#         total = len(subreddits)\n",
    "        \n",
    "#         for idx, subreddit in enumerate(subreddits, 1):\n",
    "#             print(f\"Fetching data for r/{subreddit} ({idx}/{total})\")\n",
    "            \n",
    "#             subreddit_data = self.get_subreddit_info(subreddit)\n",
    "#             if subreddit_data:\n",
    "#                 all_data.append(subreddit_data)\n",
    "            \n",
    "#             # Be nice to Reddit's servers\n",
    "#             time.sleep(2)\n",
    "        \n",
    "#         # Create DataFrame\n",
    "#         df = pd.DataFrame(all_data)\n",
    "        \n",
    "#         if save_results and not df.empty:\n",
    "#             self.save_data(df)\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     def save_data(self, df):\n",
    "#         \"\"\"Save the subreddit information to both CSV and JSON formats\"\"\"\n",
    "#         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "#         # Save as CSV\n",
    "#         csv_filename = f'subreddit_info_{timestamp}.csv'\n",
    "#         df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "#         # Save as JSON with better formatting\n",
    "#         json_filename = f'subreddit_info_{timestamp}.json'\n",
    "#         df.to_json(json_filename, orient='records', date_format='iso', indent=4)\n",
    "        \n",
    "#         print(f\"\\nData saved to:\")\n",
    "#         print(f\"CSV: {csv_filename}\")\n",
    "#         print(f\"JSON: {json_filename}\")\n",
    "\n",
    "#     def print_summary(self, df):\n",
    "#         \"\"\"Print summary statistics about the collected subreddit data\"\"\"\n",
    "#         if df.empty:\n",
    "#             print(\"No data collected!\")\n",
    "#             return\n",
    "            \n",
    "#         print(\"\\nSummary Statistics:\")\n",
    "#         print(f\"Total subreddits analyzed: {len(df)}\")\n",
    "        \n",
    "#         print(\"\\nSubscriber Statistics:\")\n",
    "#         print(df['subscribers'].describe())\n",
    "        \n",
    "#         print(\"\\nActive Users Statistics:\")\n",
    "#         print(df['active_users'].describe())\n",
    "        \n",
    "#         print(\"\\nAge of Subreddits:\")\n",
    "#         df['age_days'] = (datetime.now() - df['created_utc']).dt.days\n",
    "#         print(df['age_days'].describe())\n",
    "        \n",
    "#         print(\"\\nSubreddit Types Distribution:\")\n",
    "#         print(df['subreddit_type'].value_counts())\n",
    "        \n",
    "#         print(\"\\nFeature Availability:\")\n",
    "#         features = ['allows_images', 'allows_videos', 'allows_polls', \n",
    "#                    'allow_galleries', 'wiki_enabled', 'emojis_enabled']\n",
    "#         for feature in features:\n",
    "#             if feature in df.columns:\n",
    "#                 print(f\"{feature}: {df[feature].mean()*100:.1f}% enabled\")\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     scraper = SubredditScraper()\n",
    "    \n",
    "#     # Example list of subreddits to analyze\n",
    "#     subreddits_to_analyze = [\n",
    "#         \"python\",\n",
    "#         \"programming\",\n",
    "#         \"datascience\",\n",
    "#         \"machinelearning\",\n",
    "#         \"javascript\",\n",
    "#         \"webdev\",\n",
    "#         \"technology\",\n",
    "#         \"science\"\n",
    "#     ]\n",
    "    \n",
    "#     # Get information for all subreddits\n",
    "#     subreddits_df = scraper.get_multiple_subreddits(subreddits_to_analyze)\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     scraper.print_summary(subreddits_df)\n",
    "    \n",
    "#     # Example: Get top 5 subreddits by subscriber count\n",
    "#     top_by_subscribers = subreddits_df.nlargest(5, 'subscribers')\n",
    "#     print(\"\\nTop 5 Subreddits by Subscribers:\")\n",
    "#     print(top_by_subscribers[['display_name', 'subscribers', 'active_users']])\n",
    "    \n",
    "#     # Example: Get newest subreddits\n",
    "#     newest_subreddits = subreddits_df.nsmallest(5, 'created_utc')\n",
    "#     print(\"\\nNewest Subreddits:\")\n",
    "#     print(newest_subreddits[['display_name', 'created_utc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching subreddits... Current count: 0\n",
      "Fetching subreddits... Current count: 100\n",
      "Fetching subreddits... Current count: 200\n",
      "Fetching subreddits... Current count: 300\n",
      "Fetching subreddits... Current count: 400\n",
      "Fetching subreddits... Current count: 500\n",
      "Fetching subreddits... Current count: 600\n",
      "Fetching subreddits... Current count: 700\n",
      "Fetching subreddits... Current count: 800\n",
      "Fetching subreddits... Current count: 900\n",
      "Fetching subreddits... Current count: 1000\n",
      "Fetching subreddits... Current count: 1100\n",
      "Fetching subreddits... Current count: 1200\n",
      "Fetching subreddits... Current count: 1300\n",
      "Fetching subreddits... Current count: 1400\n",
      "Fetching subreddits... Current count: 1500\n",
      "Fetching subreddits... Current count: 1600\n",
      "Fetching subreddits... Current count: 1700\n",
      "Fetching subreddits... Current count: 1800\n",
      "Fetching subreddits... Current count: 1900\n",
      "Fetching subreddits... Current count: 2000\n",
      "Fetching subreddits... Current count: 2099\n",
      "Fetching subreddits... Current count: 2199\n",
      "Fetching subreddits... Current count: 2299\n",
      "Fetching subreddits... Current count: 2399\n",
      "Fetching subreddits... Current count: 2498\n",
      "Fetching subreddits... Current count: 2598\n",
      "Fetching subreddits... Current count: 2698\n",
      "Fetching subreddits... Current count: 2798\n",
      "Fetching subreddits... Current count: 2897\n",
      "Fetching subreddits... Current count: 2997\n",
      "Fetching subreddits... Current count: 3097\n",
      "Fetching subreddits... Current count: 3196\n",
      "Fetching subreddits... Current count: 3296\n",
      "Fetching subreddits... Current count: 3396\n",
      "Fetching subreddits... Current count: 3494\n",
      "Fetching subreddits... Current count: 3593\n",
      "Fetching subreddits... Current count: 3693\n",
      "Fetching subreddits... Current count: 3792\n",
      "Fetching subreddits... Current count: 3892\n",
      "Fetching subreddits... Current count: 3992\n",
      "Fetching subreddits... Current count: 4090\n",
      "Fetching subreddits... Current count: 4189\n",
      "Fetching subreddits... Current count: 4288\n",
      "Reached end of listings\n",
      "\n",
      "Summary Statistics:\n",
      "Total subreddits collected: 4374\n",
      "\n",
      "Subscriber Statistics:\n",
      "count    4.374000e+03\n",
      "mean     1.017135e+06\n",
      "std      5.609488e+06\n",
      "min      1.006900e+04\n",
      "25%      9.184250e+04\n",
      "50%      2.027980e+05\n",
      "75%      5.757472e+05\n",
      "max      3.072602e+08\n",
      "Name: subscribers, dtype: float64\n",
      "\n",
      "Top 10 Subreddits:\n",
      "     display_name_prefixed                                     title  \\\n",
      "1703       r/announcements                             Announcements   \n",
      "14                 r/funny                                     funny   \n",
      "1              r/AskReddit                             Ask Reddit...   \n",
      "18                r/gaming                                  r/gaming   \n",
      "19             r/worldnews                                World News   \n",
      "26         r/todayilearned                     Today I Learned (TIL)   \n",
      "211                  r/aww  A subreddit for cute and cuddly pictures   \n",
      "157                r/Music                                   r/Music   \n",
      "28                 r/memes          /r/Memes the original since 2008   \n",
      "16                r/movies                 Movie News and Discussion   \n",
      "\n",
      "      subscribers active_users  \n",
      "1703    307260183         None  \n",
      "14       64521522         None  \n",
      "1        48885940         None  \n",
      "18       43973317         None  \n",
      "19       41860438         None  \n",
      "26       38317531         None  \n",
      "211      37009750         None  \n",
      "157      35084867         None  \n",
      "28       34551394         None  \n",
      "16       33820502         None  \n",
      "\n",
      "Data saved to: subreddit_list_20241026_201202.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class SubredditListScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.collected_subreddits = set()\n",
    "\n",
    "    def fetch_subreddits(self, limit=1000, min_subscribers=1000):\n",
    "        \"\"\"\n",
    "        Fetch subreddits from Reddit\n",
    "        \n",
    "        Args:\n",
    "            limit (int): Number of subreddits to fetch\n",
    "            min_subscribers (int): Minimum number of subscribers required\n",
    "        \"\"\"\n",
    "        subreddits_data = []\n",
    "        after = None\n",
    "        \n",
    "        while len(subreddits_data) < limit:\n",
    "            try:\n",
    "                url = f'https://www.reddit.com/subreddits/popular.json?limit=100'\n",
    "                if after:\n",
    "                    url += f'&after={after}'\n",
    "                \n",
    "                print(f\"Fetching subreddits... Current count: {len(subreddits_data)}\")\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                for subreddit in data['data']['children']:\n",
    "                    subreddit_data = subreddit['data']\n",
    "                    \n",
    "                    # Skip if already collected\n",
    "                    if subreddit_data['display_name'] in self.collected_subreddits:\n",
    "                        continue\n",
    "                        \n",
    "                    # Check subscriber count\n",
    "                    if subreddit_data.get('subscribers', 0) >= min_subscribers:\n",
    "                        sub_info = {\n",
    "                            'display_name': subreddit_data.get('display_name'),\n",
    "                            'display_name_prefixed': subreddit_data.get('display_name_prefixed'),\n",
    "                            'title': subreddit_data.get('title'),\n",
    "                            'subscribers': subreddit_data.get('subscribers', 0),\n",
    "                            'active_users': subreddit_data.get('active_user_count', 0),\n",
    "                            'created_utc': datetime.fromtimestamp(subreddit_data.get('created_utc', 0)),\n",
    "                            'description': subreddit_data.get('description'),\n",
    "                            'subreddit_type': subreddit_data.get('subreddit_type'),\n",
    "                            'over18': subreddit_data.get('over18', False),\n",
    "                            'url': f\"https://www.reddit.com/r/{subreddit_data.get('display_name')}/\"\n",
    "                        }\n",
    "                        \n",
    "                        subreddits_data.append(sub_info)\n",
    "                        self.collected_subreddits.add(subreddit_data['display_name'])\n",
    "                \n",
    "                # Get pagination token\n",
    "                after = data['data'].get('after')\n",
    "                if not after:\n",
    "                    print(\"Reached end of listings\")\n",
    "                    break\n",
    "                    \n",
    "                time.sleep(2)  # Be nice to Reddit's servers\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching data: {e}\")\n",
    "                break\n",
    "                \n",
    "        return pd.DataFrame(subreddits_data)\n",
    "\n",
    "    def save_data(self, df):\n",
    "        \"\"\"Save the subreddit list to both CSV and JSON formats\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"No data to save!\")\n",
    "            return\n",
    "            \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Save as CSV\n",
    "        csv_filename = f'subreddit_list_{timestamp}.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nData saved to: {csv_filename}\")\n",
    "\n",
    "    def print_summary(self, df):\n",
    "        \"\"\"Print summary statistics about the collected subreddits\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"No data collected!\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Total subreddits collected: {len(df)}\")\n",
    "        print(\"\\nSubscriber Statistics:\")\n",
    "        print(df['subscribers'].describe())\n",
    "        \n",
    "        # Print top 10 subreddits\n",
    "        print(\"\\nTop 10 Subreddits:\")\n",
    "        top_10 = df.nlargest(10, 'subscribers')\n",
    "        print(top_10[['display_name_prefixed', 'title', 'subscribers', 'active_users']])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = SubredditListScraper()\n",
    "    \n",
    "    # Collect subreddits with at least 10,000 subscribers\n",
    "    subreddits_df = scraper.fetch_subreddits(\n",
    "        limit=10000,  # Try to collect 1000 subreddits\n",
    "        min_subscribers=10000  # Only subreddits with 10k+ subscribers\n",
    "    )\n",
    "    \n",
    "    # Print summary and save\n",
    "    scraper.print_summary(subreddits_df)\n",
    "    scraper.save_data(subreddits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to scrape 20 subreddits...\n",
      "Filtering for bottom 20 subreddits by subscriber count\n",
      "\n",
      "Selected subreddits:\n",
      "r/BetterDiscord: 10,069 subscribers\n",
      "r/DanhengMains: 10,096 subscribers\n",
      "r/NintendoSwitchBoxArt: 10,310 subscribers\n",
      "r/DownSouth: 10,570 subscribers\n",
      "r/DS4Windows: 10,895 subscribers\n",
      "r/ChioriMains: 10,928 subscribers\n",
      "r/CurseForge: 10,967 subscribers\n",
      "r/insomniacleaks: 11,193 subscribers\n",
      "r/FuckTAA: 11,343 subscribers\n",
      "r/japanpeopletwitter: 11,473 subscribers\n",
      "r/otvandfriendsrumors: 11,513 subscribers\n",
      "r/opus_magnum: 11,585 subscribers\n",
      "r/prime: 11,794 subscribers\n",
      "r/JingYuanMains: 12,447 subscribers\n",
      "r/dewrim: 12,858 subscribers\n",
      "r/byu: 12,887 subscribers\n",
      "r/bahn: 13,043 subscribers\n",
      "r/BlackSwanMains_HSR: 13,058 subscribers\n",
      "r/hospitalist: 13,093 subscribers\n",
      "r/handbrake: 13,188 subscribers\n",
      "\n",
      "Processing 1/20: r/BetterDiscord\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 2/20: r/DanhengMains\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 3/20: r/NintendoSwitchBoxArt\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 4/20: r/DownSouth\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 5/20: r/DS4Windows\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 6/20: r/ChioriMains\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 7/20: r/CurseForge\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 8/20: r/insomniacleaks\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 9/20: r/FuckTAA\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 10/20: r/japanpeopletwitter\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 11/20: r/otvandfriendsrumors\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 12/20: r/opus_magnum\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 13/20: r/prime\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 14/20: r/JingYuanMains\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 15/20: r/dewrim\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 16/20: r/byu\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 17/20: r/bahn\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 18/20: r/BlackSwanMains_HSR\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 19/20: r/hospitalist\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Processing 20/20: r/handbrake\n",
      "✓ Successfully collected 3 posts\n",
      "\n",
      "Successfully collected 60 posts from 20 subreddits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>otvandfriendsrumors</td>\n",
       "      <td>Fuslie Confirms She's The Reason For NoahJ's D...</td>\n",
       "      <td>1170</td>\n",
       "      <td>365</td>\n",
       "      <td>2024-08-07 21:26:59</td>\n",
       "      <td>https://www.reddit.com/r/otvandfriendsrumors/c...</td>\n",
       "      <td>https://www.reddit.com/r/otvandfriendsrumors/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>otvandfriendsrumors</td>\n",
       "      <td>Aaandddd she's back on Twitch</td>\n",
       "      <td>896</td>\n",
       "      <td>63</td>\n",
       "      <td>2024-07-12 18:03:41</td>\n",
       "      <td>https://i.redd.it/nec0jvnvq6cd1.png</td>\n",
       "      <td>https://www.reddit.com/r/otvandfriendsrumors/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>prime</td>\n",
       "      <td>Nobody wants to drink this shit</td>\n",
       "      <td>3502</td>\n",
       "      <td>779</td>\n",
       "      <td>2023-09-10 08:16:00</td>\n",
       "      <td>https://i.redd.it/r2x9ibd33gnb1.jpg</td>\n",
       "      <td>https://www.reddit.com/r/prime/comments/16f2s9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>prime</td>\n",
       "      <td>Oops 🤫</td>\n",
       "      <td>2836</td>\n",
       "      <td>166</td>\n",
       "      <td>2024-02-09 08:36:54</td>\n",
       "      <td>https://i.redd.it/8qhf5cd38lhc1.jpeg</td>\n",
       "      <td>https://www.reddit.com/r/prime/comments/1amsa9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>prime</td>\n",
       "      <td>Nobody is buying this shit 😭</td>\n",
       "      <td>2756</td>\n",
       "      <td>505</td>\n",
       "      <td>2024-01-16 04:43:10</td>\n",
       "      <td>https://i.redd.it/hahidejisscc1.jpeg</td>\n",
       "      <td>https://www.reddit.com/r/prime/comments/19821s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit                                              title  \\\n",
       "31  otvandfriendsrumors  Fuslie Confirms She's The Reason For NoahJ's D...   \n",
       "32  otvandfriendsrumors                      Aaandddd she's back on Twitch   \n",
       "36                prime                    Nobody wants to drink this shit   \n",
       "37                prime                                             Oops 🤫   \n",
       "38                prime                       Nobody is buying this shit 😭   \n",
       "\n",
       "    score  num_comments         created_utc  \\\n",
       "31   1170           365 2024-08-07 21:26:59   \n",
       "32    896            63 2024-07-12 18:03:41   \n",
       "36   3502           779 2023-09-10 08:16:00   \n",
       "37   2836           166 2024-02-09 08:36:54   \n",
       "38   2756           505 2024-01-16 04:43:10   \n",
       "\n",
       "                                                  url  \\\n",
       "31  https://www.reddit.com/r/otvandfriendsrumors/c...   \n",
       "32                https://i.redd.it/nec0jvnvq6cd1.png   \n",
       "36                https://i.redd.it/r2x9ibd33gnb1.jpg   \n",
       "37               https://i.redd.it/8qhf5cd38lhc1.jpeg   \n",
       "38               https://i.redd.it/hahidejisscc1.jpeg   \n",
       "\n",
       "                                            permalink  \n",
       "31  https://www.reddit.com/r/otvandfriendsrumors/c...  \n",
       "32  https://www.reddit.com/r/otvandfriendsrumors/c...  \n",
       "36  https://www.reddit.com/r/prime/comments/16f2s9...  \n",
       "37  https://www.reddit.com/r/prime/comments/1amsa9...  \n",
       "38  https://www.reddit.com/r/prime/comments/19821s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class MultiRedditScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def fetch_top_posts(self, subreddit, limit=10):\n",
    "        \"\"\"\n",
    "        Fetch top posts from a subreddit using Reddit's JSON API\n",
    "        \n",
    "        Args:\n",
    "            subreddit (str): Name of the subreddit without the 'r/'\n",
    "            limit (int): Maximum number of posts to fetch\n",
    "            \n",
    "        Returns:\n",
    "            list: List of post dictionaries\n",
    "        \"\"\"\n",
    "        posts_data = []\n",
    "        url = f'https://www.reddit.com/r/{subreddit}/top.json?t=all&limit={limit}'\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for post in data['data']['children']:\n",
    "                post_data = post['data']\n",
    "                posts_data.append({\n",
    "                    'subreddit': subreddit,\n",
    "                    'title': post_data.get('title'),\n",
    "                    'score': post_data.get('score'),\n",
    "                    'num_comments': post_data.get('num_comments'),\n",
    "                    'created_utc': datetime.fromtimestamp(post_data.get('created_utc', 0)),\n",
    "                    'url': post_data.get('url'),\n",
    "                    'permalink': 'https://www.reddit.com' + post_data.get('permalink', '')\n",
    "                })\n",
    "            \n",
    "            time.sleep(2)  # Be nice to Reddit's servers\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching posts from r/{subreddit}: {e}\")\n",
    "        \n",
    "        return posts_data\n",
    "\n",
    "    def get_top_subreddits(self, subreddits_df, top_n=None, order='top'):\n",
    "        \"\"\"\n",
    "        Filter subreddits DataFrame to get top or bottom N subreddits by subscriber count\n",
    "        \n",
    "        Args:\n",
    "            subreddits_df (pandas.DataFrame): DataFrame containing subreddit information\n",
    "            top_n (int): Number of subreddits to return. If None, returns all subreddits\n",
    "            order (str): 'top' for most popular or 'bottom' for least popular subreddits\n",
    "                \n",
    "        Returns:\n",
    "            pandas.DataFrame: Filtered DataFrame with selected subreddits\n",
    "        \"\"\"\n",
    "        if top_n is None or top_n >= len(subreddits_df):\n",
    "            return subreddits_df\n",
    "        \n",
    "        # Sort by subscribers\n",
    "        sorted_df = subreddits_df.sort_values('subscribers', \n",
    "                                            ascending=True if order == 'bottom' else False)\n",
    "        \n",
    "        # Get top or bottom N\n",
    "        return sorted_df.head(top_n).copy()\n",
    "\n",
    "    def scrape_multiple_subreddits(self, subreddits_df, posts_per_subreddit=10, top_n=None, order='top'):\n",
    "        \"\"\"\n",
    "        Scrape top posts from multiple subreddits\n",
    "        \n",
    "        Args:\n",
    "            subreddits_df (pandas.DataFrame): DataFrame containing subreddit information\n",
    "            posts_per_subreddit (int): Number of top posts to fetch per subreddit\n",
    "            top_n (int): Optional number of subreddits to scrape (by subscriber count)\n",
    "            order (str): 'top' for most popular or 'bottom' for least popular subreddits\n",
    "                \n",
    "        Returns:\n",
    "            pandas.DataFrame: Combined DataFrame of all top posts\n",
    "        \"\"\"\n",
    "        # Filter for selected subreddits\n",
    "        filtered_df = self.get_top_subreddits(subreddits_df, top_n, order)\n",
    "        total_subreddits = len(filtered_df)\n",
    "        \n",
    "        print(f\"\\nStarting to scrape {total_subreddits} subreddits...\")\n",
    "        if top_n:\n",
    "            print(f\"Filtering for {'top' if order == 'top' else 'bottom'} {top_n} subreddits by subscriber count\")\n",
    "            print(\"\\nSelected subreddits:\")\n",
    "            for _, row in filtered_df.iterrows():\n",
    "                print(f\"r/{row['display_name']}: {row['subscribers']:,} subscribers\")\n",
    "        all_posts = []\n",
    "        \n",
    "        for idx, (_, row) in enumerate(filtered_df.iterrows(), 1):\n",
    "            subreddit = row['display_name']\n",
    "            print(f\"\\nProcessing {idx}/{total_subreddits}: r/{subreddit}\")\n",
    "            \n",
    "            try:\n",
    "                posts = self.fetch_top_posts(subreddit, limit=posts_per_subreddit)\n",
    "                all_posts.extend(posts)\n",
    "                print(f\"✓ Successfully collected {len(posts)} posts\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing r/{subreddit}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if all_posts:\n",
    "            posts_df = pd.DataFrame(all_posts)\n",
    "            \n",
    "            # Sort by score and num_comments\n",
    "            posts_df = posts_df.sort_values(['subreddit', 'score', 'num_comments'], \n",
    "                                          ascending=[True, False, False])\n",
    "            \n",
    "            print(f\"\\nSuccessfully collected {len(posts_df)} posts from {posts_df['subreddit'].nunique()} subreddits\")\n",
    "        else:\n",
    "            posts_df = pd.DataFrame(columns=['subreddit', 'title', 'score', 'num_comments', \n",
    "                                           'created_utc', 'url', 'permalink'])\n",
    "            print(\"\\nNo posts were collected\")\n",
    "        \n",
    "        return posts_df\n",
    "\n",
    "    def save_results(self, posts_df, filename_prefix='top_posts'):\n",
    "        \"\"\"Save results to CSV and JSON formats\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_filename = f'{filename_prefix}_{timestamp}.csv'\n",
    "        posts_df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        # Save to JSON\n",
    "        json_filename = f'{filename_prefix}_{timestamp}.json'\n",
    "        posts_df.to_json(json_filename, orient='records', date_format='iso')\n",
    "        \n",
    "        print(f\"\\nData saved to:\")\n",
    "        print(f\"CSV: {csv_filename}\")\n",
    "        print(f\"JSON: {json_filename}\")\n",
    "        \n",
    "        return csv_filename, json_filename\n",
    "\n",
    "# Run the scraper\n",
    "scraper = MultiRedditScraper()\n",
    "\n",
    "# # For most popular subreddits\n",
    "# top_posts_df = scraper.scrape_multiple_subreddits(\n",
    "#     subreddits_df,\n",
    "#     posts_per_subreddit=10,\n",
    "#     top_n=5  # Gets 5 most popular subreddits\n",
    "# )\n",
    "\n",
    "# For least popular subreddits\n",
    "bottom_posts_df = scraper.scrape_multiple_subreddits(\n",
    "    subreddits_df,\n",
    "    posts_per_subreddit=3,\n",
    "    top_n=20,\n",
    "    order='bottom'  # Gets 5 least popular subreddits\n",
    ")\n",
    "\n",
    "bottom_posts_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching top 3 most upvoted comments for 10 posts...\n",
      "\n",
      "Processing post 52/10\n",
      "Title: Black Swan x Shorekeeper [art by McDoboArt]...\n",
      "Post Score: 4614, Number of Comments: 44\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 68 upvotes\n",
      "Comment 2: 55 upvotes\n",
      "Comment 3: 14 upvotes\n",
      "\n",
      "Processing post 49/10\n",
      "Title: Einmal passiert, seit dem buche ich niemals die letzten Züge. Egal wie günstig sie sind....\n",
      "Post Score: 3868, Number of Comments: 83\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 134 upvotes\n",
      "Comment 2: 35 upvotes\n",
      "Comment 3: 25 upvotes\n",
      "\n",
      "Processing post 37/10\n",
      "Title: Nobody wants to drink this shit...\n",
      "Post Score: 3502, Number of Comments: 779\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 46 upvotes\n",
      "Comment 2: 14 upvotes\n",
      "Comment 3: 8 upvotes\n",
      "\n",
      "Processing post 53/10\n",
      "Title: You didn't need to be so direct, Black Swan....\n",
      "Post Score: 3160, Number of Comments: 42\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 103 upvotes\n",
      "Comment 2: 70 upvotes\n",
      "Comment 3: 47 upvotes\n",
      "\n",
      "Processing post 16/10\n",
      "Title: Chiori chilling (秋葉　月見)...\n",
      "Post Score: 2882, Number of Comments: 9\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 30 upvotes\n",
      "Comment 2: 13 upvotes\n",
      "Comment 3: 5 upvotes\n",
      "\n",
      "Processing post 50/10\n",
      "Title: 35 Stunden Fahrt von LA nach Seattle...\n",
      "Post Score: 2841, Number of Comments: 132\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 29 upvotes\n",
      "Comment 2: 13 upvotes\n",
      "Comment 3: 5 upvotes\n",
      "\n",
      "Processing post 38/10\n",
      "Title: Oops 🤫...\n",
      "Post Score: 2836, Number of Comments: 166\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 195 upvotes\n",
      "Comment 2: 78 upvotes\n",
      "Comment 3: 8 upvotes\n",
      "\n",
      "Processing post 39/10\n",
      "Title: Nobody is buying this shit 😭...\n",
      "Post Score: 2756, Number of Comments: 505\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 113 upvotes\n",
      "Comment 2: 113 upvotes\n",
      "Comment 3: 44 upvotes\n",
      "\n",
      "Processing post 54/10\n",
      "Title: Spoilers: She couldn’t fix her....\n",
      "Post Score: 2622, Number of Comments: 104\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 116 upvotes\n",
      "Comment 2: 55 upvotes\n",
      "Comment 3: 52 upvotes\n",
      "\n",
      "Processing post 51/10\n",
      "Title: Bitte nicht einsteigen...\n",
      "Post Score: 2599, Number of Comments: 79\n",
      "\n",
      "Top comment scores for this post:\n",
      "Comment 1: 78 upvotes\n",
      "Comment 2: 38 upvotes\n",
      "Comment 3: 28 upvotes\n",
      "\n",
      "Verification of comment sorting:\n",
      "\n",
      "Post: Black Swan x Shorekeeper [art by McDoboArt]...\n",
      "Comment scores: [68, 55, 14]\n",
      "\n",
      "Post: Einmal passiert, seit dem buche ich niemals die letzten Züge. Egal wie günstig sie sind....\n",
      "Comment scores: [134, 35, 25]\n",
      "\n",
      "Post: Nobody wants to drink this shit...\n",
      "Comment scores: [46, 14, 8]\n",
      "\n",
      "Post: You didn't need to be so direct, Black Swan....\n",
      "Comment scores: [103, 70, 47]\n",
      "\n",
      "Post: Chiori chilling (秋葉　月見)...\n",
      "Comment scores: [30, 13, 5]\n",
      "\n",
      "Post: 35 Stunden Fahrt von LA nach Seattle...\n",
      "Comment scores: [29, 13, 5]\n",
      "\n",
      "Post: Oops 🤫...\n",
      "Comment scores: [195, 78, 8]\n",
      "\n",
      "Post: Nobody is buying this shit 😭...\n",
      "Comment scores: [113, 113, 44]\n",
      "\n",
      "Post: Spoilers: She couldn’t fix her....\n",
      "Comment scores: [116, 55, 52]\n",
      "\n",
      "Post: Bitte nicht einsteigen...\n",
      "Comment scores: [78, 38, 28]\n",
      "\n",
      "=== Detailed Statistics ===\n",
      "Files saved to:\n",
      "- CSV: reddit_comments_20241026_201422.csv\n",
      "- JSON: reddit_comments_20241026_201422.json\n",
      "\n",
      "Total unique posts: 10\n",
      "Total comments collected: 30\n",
      "Average comments per post: 3.0\n",
      "\n",
      "Comment Score Statistics:\n",
      "Maximum comment score: 195\n",
      "Average comment score: 54.4\n",
      "Minimum comment score: 5\n",
      "\n",
      "Comments per post:\n",
      "                                                    num_comments  max_score  \\\n",
      "post_title                                                                    \n",
      "35 Stunden Fahrt von LA nach Seattle                           3         29   \n",
      "Bitte nicht einsteigen                                         3         78   \n",
      "Black Swan x Shorekeeper [art by McDoboArt]                    3         68   \n",
      "Chiori chilling (秋葉　月見)                                        3         30   \n",
      "Einmal passiert, seit dem buche ich niemals die...             3        134   \n",
      "Nobody is buying this shit 😭                                   3        113   \n",
      "Nobody wants to drink this shit                                3         46   \n",
      "Oops 🤫                                                         3        195   \n",
      "Spoilers: She couldn’t fix her.                                3        116   \n",
      "You didn't need to be so direct, Black Swan.                   3        103   \n",
      "\n",
      "                                                    avg_score  min_score  \n",
      "post_title                                                                \n",
      "35 Stunden Fahrt von LA nach Seattle                     15.7          5  \n",
      "Bitte nicht einsteigen                                   48.0         28  \n",
      "Black Swan x Shorekeeper [art by McDoboArt]              45.7         14  \n",
      "Chiori chilling (秋葉　月見)                                  16.0          5  \n",
      "Einmal passiert, seit dem buche ich niemals die...       64.7         25  \n",
      "Nobody is buying this shit 😭                             90.0         44  \n",
      "Nobody wants to drink this shit                          22.7          8  \n",
      "Oops 🤫                                                   93.7          8  \n",
      "Spoilers: She couldn’t fix her.                          74.3         52  \n",
      "You didn't need to be so direct, Black Swan.             73.3         47  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_score</th>\n",
       "      <th>post_created_utc</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_created_utc</th>\n",
       "      <th>post_url</th>\n",
       "      <th>comment_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlackSwanMains_HSR</td>\n",
       "      <td>Black Swan x Shorekeeper [art by McDoboArt]</td>\n",
       "      <td>4614</td>\n",
       "      <td>2024-10-15 06:02:53</td>\n",
       "      <td>ls197id</td>\n",
       "      <td>nisan_23</td>\n",
       "      <td>https://preview.redd.it/2vlgatn48xud1.png?widt...</td>\n",
       "      <td>68</td>\n",
       "      <td>2024-10-15 06:21:04</td>\n",
       "      <td>https://i.redd.it/cakdft9f4xud1.jpeg</td>\n",
       "      <td>https://www.reddit.comhttps://www.reddit.com/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlackSwanMains_HSR</td>\n",
       "      <td>Black Swan x Shorekeeper [art by McDoboArt]</td>\n",
       "      <td>4614</td>\n",
       "      <td>2024-10-15 06:02:53</td>\n",
       "      <td>ls1dtm1</td>\n",
       "      <td>Ahrrivederci</td>\n",
       "      <td>Black Swan has two strong arguments. But I lov...</td>\n",
       "      <td>55</td>\n",
       "      <td>2024-10-15 06:49:54</td>\n",
       "      <td>https://i.redd.it/cakdft9f4xud1.jpeg</td>\n",
       "      <td>https://www.reddit.comhttps://www.reddit.com/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BlackSwanMains_HSR</td>\n",
       "      <td>Black Swan x Shorekeeper [art by McDoboArt]</td>\n",
       "      <td>4614</td>\n",
       "      <td>2024-10-15 06:02:53</td>\n",
       "      <td>ls1r5l6</td>\n",
       "      <td>HexagonII</td>\n",
       "      <td>Symmetrical docking</td>\n",
       "      <td>14</td>\n",
       "      <td>2024-10-15 08:05:29</td>\n",
       "      <td>https://i.redd.it/cakdft9f4xud1.jpeg</td>\n",
       "      <td>https://www.reddit.comhttps://www.reddit.com/r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                   post_title  \\\n",
       "0  BlackSwanMains_HSR  Black Swan x Shorekeeper [art by McDoboArt]   \n",
       "1  BlackSwanMains_HSR  Black Swan x Shorekeeper [art by McDoboArt]   \n",
       "2  BlackSwanMains_HSR  Black Swan x Shorekeeper [art by McDoboArt]   \n",
       "\n",
       "   post_score    post_created_utc comment_id comment_author  \\\n",
       "0        4614 2024-10-15 06:02:53    ls197id       nisan_23   \n",
       "1        4614 2024-10-15 06:02:53    ls1dtm1   Ahrrivederci   \n",
       "2        4614 2024-10-15 06:02:53    ls1r5l6      HexagonII   \n",
       "\n",
       "                                        comment_body  comment_score  \\\n",
       "0  https://preview.redd.it/2vlgatn48xud1.png?widt...             68   \n",
       "1  Black Swan has two strong arguments. But I lov...             55   \n",
       "2                                Symmetrical docking             14   \n",
       "\n",
       "  comment_created_utc                              post_url  \\\n",
       "0 2024-10-15 06:21:04  https://i.redd.it/cakdft9f4xud1.jpeg   \n",
       "1 2024-10-15 06:49:54  https://i.redd.it/cakdft9f4xud1.jpeg   \n",
       "2 2024-10-15 08:05:29  https://i.redd.it/cakdft9f4xud1.jpeg   \n",
       "\n",
       "                                         comment_url  \n",
       "0  https://www.reddit.comhttps://www.reddit.com/r...  \n",
       "1  https://www.reddit.comhttps://www.reddit.com/r...  \n",
       "2  https://www.reddit.comhttps://www.reddit.com/r...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class RedditCommentScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def prepare_posts_df(self, posts_df, top_n_posts=None, sort_by=['score', 'num_comments'], \n",
    "                        ascending=False, min_score=None, min_comments=None):\n",
    "        \"\"\"\n",
    "        Sort and filter the posts dataframe before fetching comments\n",
    "        \"\"\"\n",
    "        df = posts_df.copy()\n",
    "        \n",
    "        if min_score is not None:\n",
    "            df = df[df['score'] >= min_score]\n",
    "        if min_comments is not None:\n",
    "            df = df[df['num_comments'] >= min_comments]\n",
    "        \n",
    "        df = df.sort_values(by=sort_by, ascending=ascending)\n",
    "        df = df.drop_duplicates(subset=['title'])\n",
    "        \n",
    "        if top_n_posts is not None:\n",
    "            df = df.head(top_n_posts)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def extract_comment_data(self, comment, post_info):\n",
    "        \"\"\"Extract relevant data from a comment\"\"\"\n",
    "        return {\n",
    "            'subreddit': post_info['subreddit'],\n",
    "            'post_title': post_info['title'],\n",
    "            'post_score': post_info['score'],\n",
    "            'post_created_utc': post_info['created_utc'],\n",
    "            'comment_id': comment['data'].get('id'),\n",
    "            'comment_author': comment['data'].get('author'),\n",
    "            'comment_body': comment['data'].get('body'),\n",
    "            'comment_score': comment['data'].get('score', 0),\n",
    "            'comment_created_utc': datetime.fromtimestamp(comment['data'].get('created_utc', 0)),\n",
    "            'post_url': post_info['url'],\n",
    "            'comment_url': f\"https://www.reddit.com{post_info['permalink']}{comment['data'].get('id')}\",\n",
    "        }\n",
    "    \n",
    "    def fetch_top_comments(self, post_df, num_comments=10):\n",
    "        \"\"\"\n",
    "        Fetch top comments for each post in the dataframe, sorted by upvotes\n",
    "        \"\"\"\n",
    "        all_comments = []\n",
    "        total_posts = len(post_df)\n",
    "        \n",
    "        print(f\"\\nFetching top {num_comments} most upvoted comments for {total_posts} posts...\")\n",
    "        \n",
    "        for idx, post in post_df.iterrows():\n",
    "            print(f\"\\nProcessing post {idx + 1}/{total_posts}\")\n",
    "            print(f\"Title: {post['title'][:100]}...\")\n",
    "            print(f\"Post Score: {post['score']}, Number of Comments: {post['num_comments']}\")\n",
    "            \n",
    "            try:\n",
    "                json_url = post['permalink'].replace('https://www.reddit.com', '') + '.json'\n",
    "                url = f'https://www.reddit.com{json_url}'\n",
    "                \n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if len(data) > 1:\n",
    "                    comments_data = data[1]['data']['children']\n",
    "                    \n",
    "                    # Filter out non-comment entries and extract scores\n",
    "                    valid_comments = [\n",
    "                        comment for comment in comments_data \n",
    "                        if comment['kind'] == 't1' and comment['data'].get('score') is not None\n",
    "                    ]\n",
    "                    \n",
    "                    # Sort comments by score (upvotes) in descending order\n",
    "                    sorted_comments = sorted(\n",
    "                        valid_comments,\n",
    "                        key=lambda x: x['data'].get('score', 0),\n",
    "                        reverse=True\n",
    "                    )\n",
    "                    \n",
    "                    # Take only the top N comments\n",
    "                    top_comments = sorted_comments[:num_comments]\n",
    "                    \n",
    "                    # Print comment scores for verification\n",
    "                    print(\"\\nTop comment scores for this post:\")\n",
    "                    for i, comment in enumerate(top_comments, 1):\n",
    "                        score = comment['data'].get('score', 0)\n",
    "                        print(f\"Comment {i}: {score} upvotes\")\n",
    "                    \n",
    "                    # Add to main list\n",
    "                    for comment in top_comments:\n",
    "                        all_comments.append(self.extract_comment_data(comment, post))\n",
    "                \n",
    "                time.sleep(2)\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching comments for post {idx + 1}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        # Create DataFrame and sort\n",
    "        comments_df = pd.DataFrame(all_comments)\n",
    "        \n",
    "        if not comments_df.empty:\n",
    "            # Verify sorting by showing top comments for each post\n",
    "            print(\"\\nVerification of comment sorting:\")\n",
    "            for post_title in comments_df['post_title'].unique():\n",
    "                post_comments = comments_df[comments_df['post_title'] == post_title]\n",
    "                print(f\"\\nPost: {post_title[:100]}...\")\n",
    "                print(\"Comment scores:\", post_comments['comment_score'].tolist())\n",
    "        \n",
    "        return comments_df\n",
    "    \n",
    "    def save_comments(self, comments_df, filename_prefix='reddit_comments'):\n",
    "        \"\"\"Save comments and print detailed statistics\"\"\"\n",
    "        if comments_df.empty:\n",
    "            print(\"No comments to save!\")\n",
    "            return\n",
    "            \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Save files\n",
    "        csv_filename = f'{filename_prefix}_{timestamp}.csv'\n",
    "        json_filename = f'{filename_prefix}_{timestamp}.json'\n",
    "        comments_df.to_csv(csv_filename, index=False)\n",
    "        comments_df.to_json(json_filename, orient='records', date_format='iso')\n",
    "        \n",
    "        # Print detailed statistics\n",
    "        print(\"\\n=== Detailed Statistics ===\")\n",
    "        print(f\"Files saved to:\\n- CSV: {csv_filename}\\n- JSON: {json_filename}\")\n",
    "        print(f\"\\nTotal unique posts: {comments_df['post_title'].nunique()}\")\n",
    "        print(f\"Total comments collected: {len(comments_df)}\")\n",
    "        print(f\"Average comments per post: {len(comments_df) / comments_df['post_title'].nunique():.1f}\")\n",
    "        \n",
    "        # Print comment score statistics\n",
    "        print(\"\\nComment Score Statistics:\")\n",
    "        print(f\"Maximum comment score: {comments_df['comment_score'].max()}\")\n",
    "        print(f\"Average comment score: {comments_df['comment_score'].mean():.1f}\")\n",
    "        print(f\"Minimum comment score: {comments_df['comment_score'].min()}\")\n",
    "        \n",
    "        # Show distribution of comments per post\n",
    "        print(\"\\nComments per post:\")\n",
    "        comments_per_post = comments_df.groupby('post_title').agg({\n",
    "            'comment_score': ['count', 'max', 'mean', 'min']\n",
    "        }).round(1)\n",
    "        comments_per_post.columns = ['num_comments', 'max_score', 'avg_score', 'min_score']\n",
    "        print(comments_per_post)\n",
    "\n",
    "def get_top_comments(posts_df, num_comments=10, top_n_posts=None, \n",
    "                    sort_by=['score', 'num_comments'], ascending=False,\n",
    "                    min_score=None, min_comments=None, save_output=True):\n",
    "    \"\"\"\n",
    "    Get top comments for posts, sorted by upvotes\n",
    "    \"\"\"\n",
    "    scraper = RedditCommentScraper()\n",
    "    \n",
    "    filtered_posts = scraper.prepare_posts_df(\n",
    "        posts_df,\n",
    "        top_n_posts=top_n_posts,\n",
    "        sort_by=sort_by,\n",
    "        ascending=ascending,\n",
    "        min_score=min_score,\n",
    "        min_comments=min_comments\n",
    "    )\n",
    "    \n",
    "    comments_df = scraper.fetch_top_comments(filtered_posts, num_comments)\n",
    "    \n",
    "    if save_output:\n",
    "        scraper.save_comments(comments_df)\n",
    "    \n",
    "    return comments_df\n",
    "\n",
    "\n",
    "# Get top 10 most upvoted comments for each of the top 5 highest-scoring posts\n",
    "top_comments_df = get_top_comments(\n",
    "    bottom_posts_df,\n",
    "    num_comments=3,         # Get top 10 comments per post\n",
    "    top_n_posts=10,          # Process only top 5 posts\n",
    "    sort_by='score',        # Sort posts by score\n",
    "    ascending=False,        # Sort in descending order\n",
    "    min_score=100,         # Only posts with score >= 100\n",
    "    min_comments=5         # Only posts with >= 5 comments\n",
    ")\n",
    "top_comments_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of first aggregated post:\n",
      "================================================================================\n",
      "30 years of \"freedom\" in South Africa 🇿🇦 - and locals are fighting for water in the street. The ANC has failed... yet the people will still vote for them.\n",
      "\n",
      "COMMENT (56 upvotes): Its clearly the white persons fault. Don't know where he is but when we find him he's got a lot of explaining to do.\n",
      "\n",
      "COMMENT (49 upvotes): Such an apt metaphor of South Africa. Fighting over limited resources, and wasting them in the process\n",
      "\n",
      "COMMENT (31 upvotes): It's truly disappointing when a community can't get together to get through a water crisis. \n",
      "\n",
      "Greed and selfishness is a disease that thrives in our country at the moment, both on the streets and in government.\n",
      "\n",
      "\n",
      "\n",
      "Total number of posts: 42\n"
     ]
    }
   ],
   "source": [
    "def aggregate_post_comments(df):\n",
    "    \"\"\"\n",
    "    Aggregate post titles and comments into a list of strings,\n",
    "    one element per post containing the title and all its comments.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing post_title and comment_body\n",
    "        \n",
    "    Returns:\n",
    "        list: List of strings, each containing a post's title and its comments\n",
    "    \"\"\"\n",
    "    aggregated_texts = []\n",
    "    \n",
    "    # Group by post_title\n",
    "    grouped = df.groupby('post_title')\n",
    "    \n",
    "    for title, group in grouped:\n",
    "        # Start with the post title\n",
    "        post_text = f\"{title}\\n\\n\"\n",
    "        \n",
    "        # Add each comment with its score\n",
    "        comments = group.sort_values('comment_score', ascending=False)\n",
    "        \n",
    "        for idx, row in comments.iterrows():\n",
    "            comment_text = row['comment_body']\n",
    "            comment_score = row['comment_score']\n",
    "            if pd.notna(comment_text):  # Check if comment is not NaN\n",
    "                post_text += f\"COMMENT ({comment_score} upvotes): {comment_text}\\n\\n\"\n",
    "        \n",
    "        aggregated_texts.append(post_text)\n",
    "    \n",
    "    return aggregated_texts\n",
    "\n",
    "# Example usage:\n",
    "aggregated_posts = aggregate_post_comments(top_comments_df)\n",
    "\n",
    "# Print the first post as an example\n",
    "if aggregated_posts:\n",
    "    print(\"Example of first aggregated post:\")\n",
    "    print(\"=\"*80)\n",
    "    print(aggregated_posts[0])\n",
    "    print(\"\\nTotal number of posts:\", len(aggregated_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30 years of \"freedom\" in South Africa 🇿🇦 - and locals are fighting for water in the street. The ANC has failed... yet the people will still vote for them.\\n\\nCOMMENT (56 upvotes): Its clearly the white persons fault. Don\\'t know where he is but when we find him he\\'s got a lot of explaining to do.\\n\\nCOMMENT (49 upvotes): Such an apt metaphor of South Africa. Fighting over limited resources, and wasting them in the process\\n\\nCOMMENT (31 upvotes): It\\'s truly disappointing when a community can\\'t get together to get through a water crisis. \\n\\nGreed and selfishness is a disease that thrives in our country at the moment, both on the streets and in government.\\n\\n',\n",
       " \"Animal Crossing: New Horizons Slipcover\\n\\nCOMMENT (7 upvotes): Something I put together in anticipation. Never printed a slipcover, so I can't answer any of those kinds of questions.\\n\\nCOMMENT (3 upvotes): I wonder what inspired you :P\\nLooks great\\n\\nCOMMENT (1 upvotes): Do you have the original cover\\n\\n\",\n",
       " \"Announcement from BetterDiscord discord\\n\\nCOMMENT (31 upvotes): My fonts seem not to work anymore and discord seems to be unusable now. It's honestly impressive how it's possible to design a font in such a way that makes people dizzy just from looking at it.\\n\\nEdit. Fonts work in .5 update people. We are saved.(I don't remember first two version numbers but just get the newest one)\\n\\nCOMMENT (22 upvotes): so thats why my theme broke this morning...\\n\\nCOMMENT (10 upvotes): So this is why my shit broke entirely last night after PC reboot. God damnit. I wonder how long approx it'll take. I have friends that have microphones that are super quiet so bettervolume is a big must for me.\\n\\n\",\n",
       " 'Clueless\\n\\nCOMMENT (138 upvotes): Oh, sweet summer child\\n\\nCOMMENT (110 upvotes): This is so innocently ignorant it’s cute lol\\n\\nCOMMENT (57 upvotes): They are probably FF main, thus they play game on easy mode already. Soooo pfp checks out /j\\n\\n',\n",
       " \"Cold hard facts. ANC must fall.\\n\\nCOMMENT (73 upvotes): Fokin beautifully said\\n\\nCOMMENT (62 upvotes): Who's this lady? I like her.\\n\\nCOMMENT (31 upvotes): i don't vote but for i would vote, in a 1:58 min video she stated all the issues we have in SA\\n\\n\",\n",
       " 'DHIL New Figurine Announcement!\\n\\nCOMMENT (60 upvotes): Damn I’d love one, but no way can I afford it.\\n\\nCOMMENT (29 upvotes): DHIL looking majestic as usual\\n\\nCOMMENT (26 upvotes): He’s so beautiful I’m going to cry\\n\\n',\n",
       " \"Does anyone know a theme that brings back the old icons so that I don't have to live with this? Definitely not a fan of the new ones.\\n\\nCOMMENT (25 upvotes): It exists, just add the line: \\n\\n@import url(https://davart154.github.io/Themes/Icon%20Revert%202023/2023%20Icon%20Revert.css);\\n\\nto your custom CSS or into an already existing theme (assuming the theme does not change the icons itself)\\n\\nCOMMENT (9 upvotes): yeah,  they suck, i hope someone makes a theme for changing them to the older ones\\n\\nCOMMENT (6 upvotes): The theme I use still contains plenty custom icons, so the injection is definitely possible. I hope someone makes a plugin/theme for this.\\n\\n\",\n",
       " 'Everything for Dan Heng\\n\\nCOMMENT (45 upvotes): I just want to know the eidelon and sig LC situation in the first team...\\n\\nCOMMENT (19 upvotes): My dream team..\\n\\nCOMMENT (15 upvotes): I applaud thy conviction, sibling.\\n\\n',\n",
       " 'Fix for better discord crashing when opening settings!\\n\\nCOMMENT (6 upvotes): Uninstalling till its fixed lol\\n\\nCOMMENT (3 upvotes): This fix worked, but ZeresPluginLibrary and XenoLib keeps crashing discord everytime i try to open discord\\n\\nCOMMENT (3 upvotes): All the \"worked\" comments look like bots so let me tell you brothers and sister this is not a virus (atleast I think so) and it helped me solve the problem.\\n\\n',\n",
       " 'Guys! Is my attack too low????\\n\\nCOMMENT (230 upvotes): Bro attacks 1 time per patch.\\n\\nCOMMENT (109 upvotes): I mean, you would at the VERY LEAST need 30k for any decent damage, so I would say try harder\\n\\nCOMMENT (47 upvotes): Not even a 100k, trash\\n\\n\\nSeriously tho, is that some curio in swarm disaster?\\n\\n',\n",
       " \"Has he been powercreeped already\\n\\nCOMMENT (372 upvotes): He is immune to powercreep because he is my wife\\n\\nCOMMENT (138 upvotes): Nah have you seen prydwen and cn clear speeds? He is currently faster than acheron\\n\\nCOMMENT (104 upvotes): He is actually still meta, and given the relatively low investment you gotta do for his teams in order for him to do massive amounts of damage in each turn he has compared to Acheron.\\n\\n\\nAprox of 150k-250k per turn at E0S1 without Sparkle or HuoHuo in my personal experience, 300k-420k each turn with Sparkle, and that increases to 330k-450k with Sparkle and HuoHuo, given he can use more times his ult and deal more damage with this. \\n\\n\\nI'm guessing those numbers are able to x2 with E2, which i'm planning to get once i have my E2S1 Acheron team complete.\\n\\n\",\n",
       " \"Here!\\n\\nCOMMENT (10 upvotes): I can't even get my betterdiscord to load up, let alone show this message\\n\\nCOMMENT (2 upvotes): It's working rn on canary.. so..\\n\\nCOMMENT (2 upvotes): If you really want to use BD, you can download displunger, go to an older version, the compile an older version of BD from source and install. It's fairly complex though, so I wouldn't recommend it if you don't know what you are doing.\\n\\n\",\n",
       " \"Hey! I've create a box art for Pokemon Shield inspired on old gameboy games\\n\\nCOMMENT (6 upvotes): Link to download  \\nShield: [https://1drv.ms/b/s!AvXOYKnLRpyRhKgXOvT7npT\\\\_SmRLqA?e=IL64po](https://1drv.ms/b/s!AvXOYKnLRpyRhKgXOvT7npT_SmRLqA?e=IL64po)  \\nSword: [https://1drv.ms/b/s!AvXOYKnLRpyRhKsr\\\\_aj5UqFndZ58yA?e=7A2JDt](https://1drv.ms/b/s!AvXOYKnLRpyRhKsr_aj5UqFndZ58yA?e=7A2JDt)\\n\\nCOMMENT (5 upvotes): Love this cover!\\n\\nHave you considered making a matching one for Pokémon Sword?\\n\\nCOMMENT (3 upvotes): Surprised this doesn't have more upvotes. This looks good!\\n\\n\",\n",
       " \"I crocheted Dan Heng IL\\n\\nCOMMENT (34 upvotes): GAHHHHHH HE LOOKS ADORABLE........ even the red eyeliner is there i think i'm cryinf he looks so cute and tiny i.....\\n\\nCOMMENT (18 upvotes): This so cute omg! You’re so talented.\\n\\nCOMMENT (14 upvotes): I will pay for one, this is amazing\\n\\n\",\n",
       " \"I didn't like the Mario 3D All-Stars Cover, so I made one with the Magician Mario from the original All-Stars.\\n\\nCOMMENT (4 upvotes): Good! Are you continue to work on the cover and make it available for everyone to download it?\\n\\nCOMMENT (2 upvotes): I definitely want this one.\\n\\nCOMMENT (2 upvotes): Huge improvement!\\n\\n\",\n",
       " \"I dont feel rlly safe to post this on r/discordapp but wtf?\\n\\nCOMMENT (47 upvotes): happened to almost everyone you most likely didnt get banned its just temporary\\n\\nCOMMENT (11 upvotes): Seems to be discord today..... both a friend of mine in Germany and myself had this screen today.  Some of the channels I am part of also are struggling to load up the messages... Guess there are some server issues going on.\\n\\nMobile phone seems to be operating without any issue though... not sure if that helps\\n\\nCOMMENT (5 upvotes): This also happened to me, so I try to open discord in browser. It's actually work but when I try to download Discord once again it's still block me. Does it probably just ban from using the apps but not the account?\\n\\n\",\n",
       " \"I feel like a monster\\n\\nCOMMENT (8 upvotes): The sexual content warning on the back makes this 10 times better\\n\\nCOMMENT (5 upvotes): E for Everyone\\n\\nCOMMENT (4 upvotes): Shouldn't he have angry eyebrows since this is NA art?\\n\\n\",\n",
       " 'I got 2 (and 1 fraud)\\n\\nCOMMENT (120 upvotes): damn wind dan is not a fraud 😔 still congrats!\\n\\nCOMMENT (28 upvotes): U convinced me to go all in on dhil now\\n\\nCOMMENT (24 upvotes): Respect Dan heng\\n\\n',\n",
       " \"I see a 4* Dan Heng main out in the wild and it’s like catching a glimpse of an endangered species\\n\\nCOMMENT (133 upvotes): I can’t believe 4* Dan Heng died of ligma, he will be missed 😔\\n\\nCOMMENT (58 upvotes): Welcome to the club! He's still my main DPS. I do have a team for IL but regular Dan Heng is my main.\\n\\nCOMMENT (29 upvotes): I have 5* and 4* Dan Heng both build (I use Blade and 4* Dan Heng in the same team in MoC)\\n\\n\",\n",
       " 'Inside the Sphere in Las Vegas. Could this be done in South Africa?\\n\\nCOMMENT (85 upvotes): If SA had this, we’d advance from constant loadshedding to perpetual blackout.  And of course Zim would go dark too.\\n\\nCOMMENT (47 upvotes): With ESKOM??? pfffft\\n\\nSphere probably needs more power than ESKOM produces in a week XD\\n\\nCOMMENT (13 upvotes): I\\'d wager that the shell would be built, but there would just be a normal projector screen in front because the tender \"was given to a friend\", right before it can only be used between certain time slots due to loadshedding\\n\\n',\n",
       " 'Links awakening cover with original release artwork\\n\\nCOMMENT (8 upvotes): Hey, this is my first post. Let me know what you think, I love criticism. I also love the old artwork for this game.\\n\\nCOMMENT (2 upvotes): Very good for your first box art. Only thing I would add is a white border around the screenshots\\n\\nCOMMENT (1 upvotes): This is awesome. How do I go about printing this? Like what paper and special print settings etc?\\n\\n',\n",
       " \"Lonk's Awakening (first attempt at making box art)\\n\\nCOMMENT (12 upvotes): I love it.\\n\\nCOMMENT (5 upvotes): Oof this is awesome xD wish we could mod lonk into the game\\n\\nCOMMENT (4 upvotes): literally all-time-best cover art posted to this sub.\\n\\n\",\n",
       " 'Look what they have to do to mimic a fraction of our power\\n\\nCOMMENT (35 upvotes): Only with nitro :)\\n\\nCOMMENT (18 upvotes): pathetic. this would be worth nitro while you could do it illegally.\\n\\nCOMMENT (17 upvotes): ironically , this reminded me of betterdiscord so i ended up installing it today.\\n\\n',\n",
       " 'Meanwhile in South Africa. White students got attacked by African foreigners, some foreigners told the White South Africans to leave Africa and go to Europe.\\n\\nCOMMENT (131 upvotes): Red dudes got hands\\n\\nCOMMENT (121 upvotes): Well, in my area, my neighbors are foreigners and tend to be noisy. I went out at around 11 pm and politely asked them to keep the noise down as I wanted to sleep. \\n\\nI was told to bring back the land and to go back to my own country.\\n\\nIt was a catalyst for why I am very anti-immigrant now\\n\\nCOMMENT (44 upvotes): And the oke landed on his naartjie after one shot\\n\\n',\n",
       " \"My Version of the Super Mario 3D All-Stars BoxArt, i made this to look more like the original Super Mario All-Stars BoxArt, this took me like 7 hours, but i hope you like it!\\n\\nCOMMENT (5 upvotes): Absolutely love how colorful this is! Just makes me very happy looking at it lol good work man :)\\n\\nCOMMENT (4 upvotes): Yo this is so dope !! I just wish I knew how to actually get this as a cover\\n\\nCOMMENT (4 upvotes): I actually hoped some folks on Reddit would do that.\\n\\nVery nice one ! I'll print it once I'll receive the game :)\\n\\n\",\n",
       " 'NINTENDO DIRECT PARTNER SHOWCASE LEAK\\n\\nCOMMENT (40 upvotes): Resident Evil 4! Hell yeah\\n\\n&gt; Cloud\\n\\nHell no\\n\\nCOMMENT (14 upvotes): i hate how this is just barely believable but 1 haha gta 5 would never be on switch and 2. i feel like its just a bit too many games for a partner showcase.\\n\\nCOMMENT (8 upvotes): Honestly just getting Hi-Fi Rush and Stray on Switch would be more than enough to get me hyped.\\n\\n',\n",
       " \"Nacon Wireless Controler is not found in ds4windows\\n\\nCOMMENT (2 upvotes): Bro, get rid of this crap and buy normal PS controller. It's expensive? Hmm what have you spent viewers' money on?\\n\\nCOMMENT (1 upvotes): Literal stick\\n\\nCOMMENT (1 upvotes): Controller\\\\*\\n\\n\",\n",
       " \"Nintendo Switch Cover Template! (.psd file!)\\n\\nCOMMENT (28 upvotes): It's easy for anyone to customize, even beginners! It's the correct resolution so that you don't have to worry about resizing anything, just print and you're ready to go! Type the game title, select an ESRB rating, add pro controller support, set the players, and more! \\n\\n\\nDoesn't look exactly how you want? No problem, it's easy to move and shift things around to make it as authentic as possible. I even will put my scans into this template, so that everything is the same consistent quality! \\n\\nI hope this will inspire more quality customs and nice scans!\\n\\n[**-&gt; Here's the Google Drive link to the download! &lt;-**](https://drive.google.com/drive/folders/1lcIfNN_cBmPDYIlfidd8yQiXY_jVac_4?usp=sharing)\\n\\nThis is a .psd file, which requires the fonts MarkPro and Roboto, both which are included in the drive folder. It will probably work decently in other programs, due to .psd files starting to become fairly universal, but I can't test it with everything.\\n\\n**Enjoy, I hope it's helpful!**\\n\\nCOMMENT (6 upvotes): This is absolutely amazing, thank you. Any chance this post can be stickied or added to sidebar at least?\\n\\nCOMMENT (2 upvotes): Glad this was posted. Hope more people are inspired to make box art\\n\\n\",\n",
       " \"Playstation dualshock 4 controller keeps disconnecting from PC's bluetooth\\n\\nCOMMENT (2 upvotes): Mine kept disconnecting also but I realized it was picking up as an audio device and a mouse,keyboard, and pen device. I removed it from an audio device and it hasn't disconnected since.\\n\\nCOMMENT (1 upvotes): Any updates on this?\\n\\nSame problem happens to me, I always have to remove the BT connection (Wireless Controller) and connect again after every time I use the controller. The DS4 program doesn't even recognize the controller, so I can't do anything there. If anyone here has any clue.\\n\\nCOMMENT (1 upvotes): Yeah so mine didn’t work. Removed all Bluetooth devices and replaced my Bluetooth dongle. Also uninstalled and reinstalled vigem bus driver since that wasn’t working. Strange things happening though it’s being picked up as both controller and hands free device now and I made it worse. Just restarted PC.. it didn’t work.\\n\\n\",\n",
       " 'Putting Dan Heng IL on Dr. Ratio’s LC (My Art)\\n\\nCOMMENT (71 upvotes): AWOOGA\\n\\nCOMMENT (33 upvotes): Okay, this really looks fine on Daniel edit\\n\\nCOMMENT (22 upvotes): Canon I was the book\\n\\n',\n",
       " 'Should white people give back the land they took during apartheid?\\n\\nCOMMENT (110 upvotes): Will Nguni people give back the land they took from the Khoi and San peoples ?\\n\\nCOMMENT (55 upvotes): There is no need for these moralistic arguments about whether black people are fit to own land or run the country.  Those are nonsensical arguments that often ignore context or are made in bad faith.   Often, these arguments make racist inferences, anyway.   The only valid argument is this:\\n\\nWhat does \"took during apartheid\" mean?  If you bought land, which is what everyone did back in the day, then you didn\\'t take it from anyone.   The buyer isn\\'t responsible for anything that happened before they buy the property.   This means that no land owned by any private citizen should be the subject of land reform.  There is no need for that. \\n\\nAn argument can be made that, since the state is able to expropriate land and is the custodian of all public land, the state can be expected to \\'return\\' land they \\'took\\'.   But it is simply not possible for a private citizen to have \\'taken\\' anyone\\'s land.  If you bought land, if your name is on the title deed, then you are the only rightful owner of that land.\\n\\nCOMMENT (46 upvotes): Well, I paid for my land. They can have it back if they pay for it. Not like there\\'s a section in Absa or Remax that gives you your allocated piece of land when you show your white privilege pass.\\xa0 Reminds me, I need to get mine renewed, don\\'t want to miss out on all those privileges.\\xa0\\n\\n',\n",
       " \"Terminally ill Jacob Zuma who can't stand trial.\\n\\nCOMMENT (110 upvotes): I can’t wait to dance like that on his grave one day.\\n\\nCOMMENT (57 upvotes): I mean... Why can't we show the court this video as proof he's lying? I mean... Innocent till proven guilty right? So what am I missing here?\\n\\nCOMMENT (52 upvotes): This poes needs to go to jail\\n\\n\",\n",
       " 'The Legend of Zelda: Breath of the Wild, loving my new cover.\\n\\nCOMMENT (4 upvotes): Dope\\n\\nCOMMENT (4 upvotes): Here you can find the original artwork :)\\n\\n\\\\-&gt; [https://www.deviantart.com/salvamakoto/art/Breath-of-the-Wild-658372885](https://www.deviantart.com/salvamakoto/art/Breath-of-the-Wild-658372885)\\n\\nCOMMENT (3 upvotes): It looks amazing!!!\\n\\n',\n",
       " 'This is South Africa. You can see how defeated this man feels, he is just trying to be a good law abiding citizen and these criminals just take advantage. This is the legacy of Bheki Cele.\\n\\nCOMMENT (117 upvotes): With very low conviction rate, they will probably just get away with it.\\nSA is the wild west\\n\\nCOMMENT (115 upvotes): Thanks ANC and ANC voters.\\n\\nCOMMENT (60 upvotes): Legacy of the ANC?\\nFuck off ANC!!\\n\\n',\n",
       " 'WHY IS DAN HENG IL GENERALLY CONSIDERED WORSE THAN ACHERON/FIREFLY?\\n\\nCOMMENT (268 upvotes): They do more damage, that\\'s kinda all there is for simple \"do damage\" characters like these 3.\\n\\nCOMMENT (180 upvotes): As others said, they do more damage with lower investment but it’s also thanks to time. They’re basically the shinier new toys- same reason why you don’t hear much about Jingliu anymore.\\n\\nCOMMENT (81 upvotes): welcome to powercreep\\n\\nbut really its all about the current meta. if we go back to basic attack meta, they\\'ll be shot down a few tiers.\\n\\n',\n",
       " 'What do you get when you mix a Lamborghini and the Atlantis Dunes\\n\\nCOMMENT (10 upvotes): Its the new Lambo Huracan Sterrato (off-road)\\n\\nCOMMENT (5 upvotes): An erection.\\n\\nCOMMENT (4 upvotes): An incredibly expensive service cost.\\n\\n',\n",
       " 'What living with the ANC is like\\n\\nCOMMENT (48 upvotes): Bro needs to train his dog\\n\\nCOMMENT (25 upvotes): [deleted]\\n\\nCOMMENT (16 upvotes): That’s tru this is exactly how our old , 1 foot in the grave anc members behave for tenders\\n\\n',\n",
       " '[Solution]PS4 controller shutting down immediately after pairing\\n\\nCOMMENT (2 upvotes): Hey man, I am currently trying to figure this whole program out.\\n\\n [https://imgur.com/a/EgbBjoQ](https://imgur.com/a/EgbBjoQ) \\n\\nThese pictures show the ds4 windows and bluetooth settings in windows.\\n\\nAs you can see, in ds4 windows you can only see the bluetooth status symbol, no other details about the controller.\\n\\nNo inputs are taken from the controller in my games.\\n\\nAlso, when I hit the ps button on the controller it looks like it connects for a moment then the light goes off. I have tried uninstalling the HID drivers and installing from ds4 windows, nothing seems to work. The ds4 controller works just fine when plugged in via usb.\\n\\nCOMMENT (1 upvotes): I wish I could upvote this 1000000 times, was trying to figure this out for days, did that and it immediately worked\\n\\nEdit: do not click the \"add device\" popup you get in the corner window. If you do that, you get the secondary popups about adding the device. If your seeing those, your doing it wrong\\n\\nCOMMENT (1 upvotes): SOLVE IT!!! press share + PS button together until it hyper flashes at you. Once its hyper flashes go to settings in windows, and add bluetooth controller. Done\\n\\n',\n",
       " 'friend showed me this, someone should seriously recreate this with betterdiscord!\\n\\nCOMMENT (8 upvotes): I think you should check the themes list.\\n\\nCOMMENT (5 upvotes): is this a real thing for windows xp?\\n\\nCOMMENT (3 upvotes): I want this now\\n\\n',\n",
       " 'how to fix Emote / Gif menu from Crashing in BetterDiscord\\n\\nCOMMENT (3 upvotes): THANK YOUU!!!! i had this issue and i was so scared lmao, this helped me a lot, the relief i had! thanks mann!!\\n\\nCOMMENT (2 upvotes): this will be useless if there is going to be a update soon i would think but posting it anyway  \\nand I think the reason why this is happening because discord changed how u can only have 250 gifs.\\n\\nIf u had more than this good luck finding them back also if u lucky and you had the plugin called \"GifSaver\" you can recover your old gifs back but I don\\'t know if it works if u didn\\'t have it before the change just check to be sure and backup it as quick as u can that\\'s all.\\n\\nCOMMENT (1 upvotes): It seems BetterDiscord got a update fixing the bug so if u reading this and want the \"Emote Menu\" back On you can do now bug free lol\\n\\n',\n",
       " \"i bet you this is either gonna be a Nitro-only feature or a plugin you can find on Vencord\\n\\nCOMMENT (48 upvotes): The guy in the comments commenting their own username...\\n\\nCOMMENT (14 upvotes): nitro only 100%\\n\\nCOMMENT (8 upvotes): I don't think they've released a feature for everyone for a couple of years now everything is nitro only\\n\\n\",\n",
       " '🤷🏿\\u200d♂️\\n\\nCOMMENT (80 upvotes): Here\\'s my 5c on this matter. \\n\\nWe all as South Africans need to have a closer look, we can\\'t sheer according to this mindset anymore. There are BOTH white and black people that have insane amounts of money while there are BOTH white and black people who can\\'t afford to eat at night. We are so focused on what the better half has that we fight among ourselves, trampling each other to get to the top by any means necessary and when it comes to change, we are either to stubborn and set in our ways to speak up or we give up and give silly reasons like \"voting doesn\\'t matter\" and \"all the votes go to the ruling party anyway\".\\n\\nCOMMENT (21 upvotes): I saw someone incredibly disadvantaged today driving her brand new baby blue BMW M2.\\n\\nTough times out there everyone.\\n\\nCOMMENT (14 upvotes): I\\'m a black man from the UK and watching south African politics kills my hope we will ever get past racism cos if it hasn\\'t been sorted there it\\'s like we are all fucked. I\\'d hate to be white and in south Africa you guys are some of the moat ignored victims in the world.\\n\\n']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Invalid JSON response: ```json\n",
      "[\n",
      "    {\n",
      "        \"title\": \"PS4 controller shutting down immediately after pairing\",\n",
      "        \"...\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "Progress saved to business_ideas_20241026_1937.csv\n",
      "\n",
      "Processing complete!\n",
      "Results saved to: business_ideas_20241026_1939.csv\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import nest_asyncio\n",
    "from openai import AsyncAzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class GracefulExit(Exception):\n",
    "    pass\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise GracefulExit()\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "class ResultSaver:\n",
    "    def __init__(self):\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        self.filename = f\"business_ideas_{self.timestamp}.csv\"\n",
    "        self.results = []\n",
    "        \n",
    "    def add_result(self, content: str, ideas: List[Dict]):\n",
    "        for idea in ideas:\n",
    "            row = {\n",
    "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'original_content': content[:500],  # Truncate long content\n",
    "                'title': idea.get('title', ''),\n",
    "                'business': idea.get('business', ''),\n",
    "                'automation': json.dumps(idea.get('automation', [])),\n",
    "                'notes': json.dumps(idea.get('notes', {}))\n",
    "            }\n",
    "            self.results.append(row)\n",
    "        self.save_progress()\n",
    "    \n",
    "    def save_progress(self):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(self.filename, index=False)\n",
    "        print(f\"Progress saved to {self.filename}\")\n",
    "\n",
    "class BusinessIdeaGenerator:\n",
    "    def __init__(self, \n",
    "                 batch_size: int = 5,\n",
    "                 topic: str = \"web browsing automation\",\n",
    "                 custom_prompt: str = None):\n",
    "        self.batch_size = batch_size\n",
    "        self.rate_limiter = RateLimiter(calls_per_minute=60)\n",
    "        self.client = initialize_azure_openai_client()\n",
    "        self.topic = topic\n",
    "        self.custom_prompt = custom_prompt or self._get_default_prompt()\n",
    "        self.result_saver = ResultSaver()\n",
    "\n",
    "    def _get_default_prompt(self) -> str:\n",
    "        return \"\"\"You are a business consultant specializing in automated SaaS solutions. \n",
    "        For each Reddit post/comment provided, generate B2B SaaS business ideas that leverage web automation.\n",
    "        Each idea must:\n",
    "        1. Use web automation (Playwright/Selenium) to perform tasks without human intervention\n",
    "        2. Use AI agents for decision making\n",
    "        3. Be purely digital with no physical labor\n",
    "        4. Have a clear subscription-based revenue model\n",
    "\n",
    "        Your response must be a valid JSON array. Format each idea exactly as:\n",
    "        [\n",
    "            {\n",
    "                \"title\": \"The original Reddit post title\",\n",
    "                \"business\": \"Concise explanation of what service the business provides\",\n",
    "                \"automation\": [\n",
    "                    \"Step 1 of the automation process\",\n",
    "                    \"Step 2 of the automation process\",\n",
    "                    \"Additional steps...\"\n",
    "                ],\n",
    "                \"notes\": {\n",
    "                    \"monetization\": \"How the business makes money (pricing tiers, subscription model)\",\n",
    "                    \"industry\": \"Specific industry vertical this serves\",\n",
    "                    \"market\": {\n",
    "                        \"consumer\": \"Whether/how it serves individual consumers\",\n",
    "                        \"prosumer\": \"Whether/how it serves professional consumers\",\n",
    "                        \"enterprise\": \"Whether/how it serves enterprise clients\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        If no viable ideas can be generated, return an empty array: []\n",
    "        Focus on practical, implementable ideas using current technology.\n",
    "        Each automation step should be specific and technical, explaining exactly how web automation would accomplish the task.\"\"\"\n",
    "\n",
    "    async def generate_ideas(self, content_list: List[str]) -> List[Dict]:\n",
    "        all_ideas = []\n",
    "        try:\n",
    "            for i in range(0, len(content_list), self.batch_size):\n",
    "                batch = content_list[i:i + self.batch_size]\n",
    "                await self.rate_limiter.acquire()\n",
    "                batch_ideas = await self._process_batch(batch)\n",
    "                all_ideas.extend(batch_ideas)\n",
    "        except GracefulExit:\n",
    "            print(\"\\nGracefully shutting down and saving progress...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during idea generation: {str(e)}\")\n",
    "        finally:\n",
    "            self.result_saver.save_progress()\n",
    "        return all_ideas\n",
    "\n",
    "    async def _process_batch(self, batch: List[str]) -> List[Dict]:\n",
    "        all_batch_ideas = []\n",
    "        for content in batch:\n",
    "            try:\n",
    "                ideas = await self._generate_single_idea(content)\n",
    "                self.result_saver.add_result(content, ideas)\n",
    "                all_batch_ideas.append(ideas)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing content: {str(e)}\")\n",
    "                all_batch_ideas.append([])\n",
    "        return all_batch_ideas\n",
    "\n",
    "    async def _generate_single_idea(self, content: str) -> List[Dict]:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.custom_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Topic: {self.topic}\\n\\nContent to inspire business ideas:\\n{content}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            response = await self.client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content\n",
    "            \n",
    "            try:\n",
    "                ideas = json.loads(response_text)\n",
    "                if not isinstance(ideas, list):\n",
    "                    ideas = [ideas]\n",
    "                return ideas\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON response: {response_text[:100]}...\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating ideas: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "def generate_ideas(\n",
    "    content_list: List[str],\n",
    "    topic: str = \"web browsing automation\",\n",
    "    custom_prompt: str = None,\n",
    "    batch_size: int = 5\n",
    ") -> List[List[Dict]]:\n",
    "    async def async_generate():\n",
    "        generator = BusinessIdeaGenerator(\n",
    "            batch_size=batch_size,\n",
    "            topic=topic,\n",
    "            custom_prompt=custom_prompt\n",
    "        )\n",
    "        return await generator.generate_ideas(content_list)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(async_generate())\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        ideas = generate_ideas(\n",
    "            content_list=aggregated_posts,\n",
    "            topic=\"web browsing automation\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nProcessing complete!\")\n",
    "        print(f\"Results saved to: business_ideas_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Process failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>original_content</th>\n",
       "      <th>title</th>\n",
       "      <th>business</th>\n",
       "      <th>automation</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-26 19:37:20</td>\n",
       "      <td>30 years of \"freedom\" in South Africa 🇿🇦 - and...</td>\n",
       "      <td>web browsing automation</td>\n",
       "      <td>Automated water resource management and commun...</td>\n",
       "      <td>[\"Step 1: Use Playwright to automatically gath...</td>\n",
       "      <td>{\"monetization\": \"Subscription-based model wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-26 19:37:22</td>\n",
       "      <td>Animal Crossing: New Horizons Slipcover\\n\\nCOM...</td>\n",
       "      <td>Animal Crossing: New Horizons Slipcover</td>\n",
       "      <td>Automated custom slipcover design and printing...</td>\n",
       "      <td>[\"User uploads a game cover image or selects a...</td>\n",
       "      <td>{\"monetization\": \"Subscription model with tier...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                                   original_content  \\\n",
       "0  2024-10-26 19:37:20  30 years of \"freedom\" in South Africa 🇿🇦 - and...   \n",
       "1  2024-10-26 19:37:22  Animal Crossing: New Horizons Slipcover\\n\\nCOM...   \n",
       "\n",
       "                                     title  \\\n",
       "0                  web browsing automation   \n",
       "1  Animal Crossing: New Horizons Slipcover   \n",
       "\n",
       "                                            business  \\\n",
       "0  Automated water resource management and commun...   \n",
       "1  Automated custom slipcover design and printing...   \n",
       "\n",
       "                                          automation  \\\n",
       "0  [\"Step 1: Use Playwright to automatically gath...   \n",
       "1  [\"User uploads a game cover image or selects a...   \n",
       "\n",
       "                                               notes  \n",
       "0  {\"monetization\": \"Subscription-based model wit...  \n",
       "1  {\"monetization\": \"Subscription model with tier...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"business_ideas_20241026_1937.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Automated water resource management and community alert system',\n",
       " 'Automated custom slipcover design and printing service for video games',\n",
       " 'Automated compatibility and update management for Discord plugins and themes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['business'].to_list()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67efccac95af4a1391aab75df34da25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='business_name', options=('Automated water resource management and …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, make sure you have the required packages\n",
    "# !pip install ipywidgets matplotlib numpy pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "%matplotlib inline\n",
    "\n",
    "def analyze_business(business_name, mean=25000, std_dev=8000, business_list=None):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo simulation for a selected business\n",
    "    \"\"\"\n",
    "    # Generate simulation data\n",
    "    data = np.random.normal(mean, std_dev, 10000)\n",
    "    confidence_interval = np.percentile(data, [2.5, 97.5])\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(data, bins=30, color='#0066cc', alpha=0.7)\n",
    "    plt.axvline(mean, color='red', linestyle='--', label='Mean')\n",
    "    plt.title(f'Monthly Profit Distribution - {business_name}')\n",
    "    plt.xlabel('Profit ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nKey Statistics:\")\n",
    "    print(f\"Mean Monthly Profit: ${mean:,}\")\n",
    "    print(f\"Standard Deviation: ${std_dev:,}\")\n",
    "    print(f\"95% Confidence Interval: ${int(confidence_interval[0]):,} - ${int(confidence_interval[1]):,}\")\n",
    "\n",
    "# Create the interactive widget\n",
    "def create_interactive_analysis(business_list):\n",
    "    interact(\n",
    "        analyze_business,\n",
    "        business_name=business_list,\n",
    "        mean=(0, 100000, 1000),\n",
    "        std_dev=(1000, 50000, 1000),\n",
    "        business_list=fixed(business_list)\n",
    "    )\n",
    "\n",
    "# Run this next\n",
    "business_list = df['business'].tolist()\n",
    "create_interactive_analysis(business_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8250e9271545ac89a89c48aa7f5b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Business:', options=('Automated water resource management and community a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class BusinessAnalyzer:\n",
    "    def __init__(self, business_list):\n",
    "        self.business_list = business_list\n",
    "        self.setup_widgets()\n",
    "        \n",
    "    def setup_widgets(self):\n",
    "        # Create widgets\n",
    "        self.business_dropdown = widgets.Dropdown(\n",
    "            options=self.business_list,\n",
    "            description='Business:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.mean_slider = widgets.IntSlider(\n",
    "            value=25000,\n",
    "            min=0,\n",
    "            max=100000,\n",
    "            step=1000,\n",
    "            description='Expected Monthly Profit ($):',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.std_slider = widgets.IntSlider(\n",
    "            value=8000,\n",
    "            min=1000,\n",
    "            max=50000,\n",
    "            step=1000,\n",
    "            description='Standard Deviation ($):',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Create update button\n",
    "        self.update_button = widgets.Button(\n",
    "            description='Update Analysis',\n",
    "            button_style='primary'\n",
    "        )\n",
    "        self.update_button.on_click(self.update_plot)\n",
    "        \n",
    "        # Layout widgets\n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "    def generate_histogram_data(self):\n",
    "        mean = self.mean_slider.value\n",
    "        std_dev = self.std_slider.value\n",
    "        return np.random.normal(mean, std_dev, 10000)\n",
    "        \n",
    "    def update_plot(self, _):\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Generate data\n",
    "            data = self.generate_histogram_data()\n",
    "            confidence_interval = np.percentile(data, [2.5, 97.5])\n",
    "            \n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            # Plot histogram\n",
    "            n, bins, patches = ax.hist(data, bins=30, color='#0066cc', alpha=0.7)\n",
    "            \n",
    "            # Add mean line\n",
    "            ax.axvline(self.mean_slider.value, color='red', linestyle='--', label='Mean')\n",
    "            \n",
    "            # Customize plot\n",
    "            ax.set_title(f'Monthly Profit Distribution - {self.business_dropdown.value}')\n",
    "            ax.set_xlabel('Profit ($)')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            # Display statistics\n",
    "            print(\"\\nKey Statistics:\")\n",
    "            print(f\"Mean Monthly Profit: ${self.mean_slider.value:,}\")\n",
    "            print(f\"Standard Deviation: ${self.std_slider.value:,}\")\n",
    "            print(f\"95% Confidence Interval: ${int(confidence_interval[0]):,} - ${int(confidence_interval[1]):,}\")\n",
    "    \n",
    "    def display(self):\n",
    "        # Display all widgets\n",
    "        display(widgets.VBox([\n",
    "            self.business_dropdown,\n",
    "            self.mean_slider,\n",
    "            self.std_slider,\n",
    "            self.update_button,\n",
    "            self.output\n",
    "        ]))\n",
    "        \n",
    "        # Initial plot\n",
    "        self.update_plot(None)\n",
    "\n",
    "# Usage function\n",
    "def analyze_businesses(business_list):\n",
    "    analyzer = BusinessAnalyzer(business_list)\n",
    "    analyzer.display()\n",
    "\n",
    "# Assuming you have your DataFrame 'df' with a 'business' column\n",
    "business_list = df['business'].tolist()\n",
    "\n",
    "# Run the analysis\n",
    "analyze_businesses(business_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
